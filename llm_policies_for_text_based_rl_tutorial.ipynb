{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mauriciogtec/llm_policies_for_text_based_rl_tutorial/blob/main/llm_policies_for_text_based_rl_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzeHYa5GCxN7"
      },
      "outputs": [],
      "source": [
        "# MIT License\n",
        "#\n",
        "# @title Copyright (c) 2024 TAFM Workshop Authors (c) 2021 CCAI Community Authors { display-mode: \"form\" }\n",
        "#\n",
        "# Template modified from\n",
        "# https://colab.research.google.com/drive/1_RPUB26HWVk7SuH3OsA5FnyLx3ThaFV-?usp=sharing\n",
        "# Copyright (c) 2021 CCAI Community Authors\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13i7KQ9t-CV8"
      },
      "source": [
        "# LLM Policies for Text-based Reinforcement Learning: An Interactive Tutorial\n",
        "\n",
        "Author\n",
        "\n",
        "[**Mauricio Tec**](mauriciogtec.com), *Harvard University*, [`mauriciogtec@hsph.harvard.edu`](mailto:mauriciogtec@hsph.harvard.edu)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TL;DR**. This interactive tutorial uses LLMs for text-based reinforcement learning, considering key topics such as quantization, low-rank adaptation, fine-tuning with expert demonstrations, and reinforcement learning via proximal policy optimization."
      ],
      "metadata": {
        "id": "rMVhWY7bxfNl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNv0ANr5WcD_"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "\n",
        "*   [Overview](#overview)\n",
        "*   [Background & Prerequisites](#background-and-prereqs)\n",
        "*   [Software Requirements](#software-requirements)\n",
        "*   [Foundation Model and Task Description](#fm-task-description)\n",
        "*   [Methodology](#methodology)\n",
        "*   [Experiments & Discussion](#experiments-and-discussion)\n",
        "*   [References](#references)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH81wjfsJsv1"
      },
      "source": [
        "<a name=\"overview\"></a>\n",
        "# Overview\n",
        "\n",
        "\n",
        "**Summary**. This interactive notebook demonstrates how to train a reinforcement-learning agent in text-based environments using an LLM-parameterized policy. The ability to understand and generate natural language is essential for AI systems that need to interact with humans in a variety of domains, including customer service, healthcare, and education. By training reinforcement-learning agents in text-based environments, we can develop AI systems that are better able to understand and respond to human language, leading to more effective and versatile AI applications. The challenges faced by text-based agents are shared with other unstructured data environments, such as images and video. Investigating LLMs has implication beyond text, while maintaining the advantage of the emergence of highly capable text-based foundation models (LLMs) that can be finetuned with standard GPUs.\n",
        "\n",
        "In this tutorial, we will specifically train an agent interacting with the [Textworld (Côté et al. 2019)](https://arxiv.org/abs/1806.11532) environment, which was used in the [2019 Textworld competition](https://www.microsoft.com/en-us/research/project/textworld/competition/) organized by Microsoft Research. Other text-based environments could be considered, such as the classic [Zork game (Anderson et al., 1977)](https://en.wikipedia.org/wiki/Zork) or the Jericho’s suit of games [(Hausknecht et al., 2019)](https://arxiv.org/pdf/1909.05398). Nonetheless, Textworld provides a fast and highly parameterizable environment. We will use an easy-to-solve configuration for the purpose of this tutorial.\n",
        "\n",
        "**Target Audience**. This tutorial is designed for researchers familiar with reinforcement learning but with possibly limited hands-on experience in training and fine-tuning LLMs.\n",
        "\n",
        "**Learning Objective**. The tutorial covers key topics such as (1) parameterizing a policy using an LLM for text generation, (2) supervised fine-tuning with expert demonstrations, and (3) reinforcement learning with proximal policy optimization. The tutorial highlights strategies for efficient computation and memory management, including quantization and low-rank adaptation, which are crucial for scenarios with limited computational resources.\n",
        "\n",
        "### Contributions\n",
        "\n",
        "* It facilitates barrier to entry to RL folks\n",
        "* It highlights elements to pay attention and challenges in the area\n",
        "* Provides a useful starting code\n",
        "\n",
        "The structure of training a text-based agents bears similarities with reinforcement learning from human feedback (RLHF). However, in constrat to RLHF, the environment dynamics and reward function are externally by the environment [(Carta et al. 2023)](https://arxiv.org/pdf/2302.02662v3). This difference requires an outer loop to control the experience collection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQgijl46pYzn"
      },
      "source": [
        "<a name=\"background-and-prereqs\"></a>\n",
        "# Background & Prerequisites\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Related Work"
      ],
      "metadata": {
        "id": "K7isb1Y1xGNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Background"
      ],
      "metadata": {
        "id": "D-vg0sgwxP_X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BpQklEEIFDD"
      },
      "source": [
        "### Other References\n",
        "Feel free to include additional resources (e.g. research papers, blog posts, textbooks) for the readers to further study."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSRCNgYzUwaf"
      },
      "source": [
        "<a name=\"software-requirements\"></a>\n",
        "# Software Requirements\n",
        "Include in this section the software requirements, setup instructions, and library imports.\n",
        "\n",
        "Example:\n",
        "\n",
        "This notebook requires Python >= 3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZg6uR3Fo27C",
        "outputId": "ef6998fc-6be6-4d9b-8e5a-d1f7fb8fc4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "# print python version\n",
        "import sys\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VcTaL5BiKpu"
      },
      "source": [
        "Install dependencies. Pip will thrown an error, but it can be safely ignored. The error is due to the latest version of torchrl."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr_lxxLt3Sj1",
        "outputId": "f4f0bf3f-ff9f-465c-8395-e52cfb777ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# %pip install -q \\\n",
        "#     \"torch==2.2.1\" \\\n",
        "#     \"torchaudio==2.2.1\" \\\n",
        "#     \"flash-attn==2.5.8\" \\\n",
        "#     \"textworld-express==1.0.4\" \\\n",
        "#     \"gymnasium==0.29.1\" \\\n",
        "#     \"transformers==4.40.1\" \\\n",
        "#     \"accelerate==0.29.3\" \\\n",
        "#     \"peft==0.10.0\" \\\n",
        "#     \"bitsandbytes==0.43.1\" \\\n",
        "#     \"tqdm==4.66.2\" \\\n",
        "#     \"IProgress==0.4\" \\\n",
        "#     \"matplotlib==3.7.1\" \\\n",
        "#     \"accelerate\" \\\n",
        "#     \"trl\"\n",
        "\n",
        "%pip install -q \\\n",
        "  \"torch\" \\\n",
        "  \"torchaudio\" \\\n",
        "  \"textworld-express==1.0.4\" \\\n",
        "  \"gymnasium\" \\\n",
        "  \"tqdm==4.66.2\" \\\n",
        "  \"IProgress==0.4\" \\\n",
        "  \"matplotlib==3.7.1\" \\\n",
        "  \"git+https://github.com/huggingface/trl\" \\\n",
        "  \"git+https://github.com/huggingface/transformers\" \\\n",
        "  \"peft\" \\\n",
        "  \"accelerate\" \\\n",
        "  \"bitsandbytes\" \\\n",
        "  \"ipdb\" # for debugging\n",
        "\n",
        "# \"optimum\"\n",
        "# \"auto-gptq\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFNbeDgl3A9m"
      },
      "source": [
        "### Auxiliary functions and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhewT4SkJgQS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import transformers\n",
        "from datasets import Dataset, load_dataset\n",
        "from trl import (\n",
        "  ModelConfig,\n",
        "  SFTTrainer,\n",
        "  get_kbit_device_map,\n",
        "  get_peft_config,\n",
        "  get_quantization_config,\n",
        "  SFTConfig,\n",
        "  setup_chat_format,\n",
        ")\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig, DataCollatorForSeq2Seq\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from IPython.display import Markdown, display\n",
        "import time\n",
        "from textworld_express import TextWorldExpressEnv\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXoiLncsU3pe"
      },
      "source": [
        "<a name=\"fm-task-description\"></a>\n",
        "# Foundation Model and Task Description\n",
        "\n",
        "In this section, kindly provide a brief description of the types of foundation models that will be used (for example ChatGPT-X or Llama-X). Describe also the type of task or benchmark environment, please add a more detailed decription of the task. Feel free to provide external links and resources that discuss the specific details of the tasks or pre-trained models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh2L5-CWsJWt"
      },
      "source": [
        "### The TextWorld RL Environment\n",
        "\n",
        "The goal will be to solve a task in a cooking world virtual environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwHmSYtHslwh",
        "outputId": "43979409-fdb3-4ea8-d5fc-f472b5f8b278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**observation**: You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter that has a red apple, and a cookbook on it. In one part of the room you see a dining table, that has nothing on it. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \n",
            "To the North you see a closed sliding patio door. To the South you see a closed frosted-glass door. \n",
            "**look**: You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter that has a red apple, and a cookbook on it. In one part of the room you see a dining table, that has nothing on it. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \n",
            "To the North you see a closed sliding patio door. To the South you see a closed frosted-glass door. \n",
            "**inventory**: Inventory (maximum capacity is 4 items): \n",
            "  Your inventory is currently empty.\n",
            "\n",
            "**validActions**: ['examine trash can', 'examine oven', 'examine counter', 'examine stove', 'move south', 'examine fridge', 'take cookbook', 'open trash can', 'open door to south', 'open door to north', 'inventory', 'examine cutlery drawer', 'open cutlery drawer', 'close door to south', 'move north', 'examine dining chair', 'look around', 'examine cookbook', 'close door to north', 'open fridge', 'take red apple', 'read cookbook', 'open dishwasher', 'examine red apple', 'examine dishwasher', 'examine dining table']\n",
            "**scoreRaw**: 0.0\n",
            "**score**: 0.0\n",
            "**tasksuccess**: False\n",
            "**taskfailure**: False\n",
            "**reward**: 0\n",
            "**done**: False\n",
            "**numMoves**: 0\n",
            "**taskDescription**: You are hungry! Let's cook a delicious meal. Check the cookbook in the kitchen for the recipe. Once done, enjoy your meal!\n"
          ]
        }
      ],
      "source": [
        "STEP_LIMIT = 16\n",
        "\n",
        "def make_env(\n",
        "    game_name=\"cookingworld\",\n",
        "    step_limit=STEP_LIMIT,\n",
        "    numLocations=3,\n",
        "    includeDoors=1,\n",
        "    numDistractorItems=2,\n",
        "    numIngredients=2,\n",
        "    **kwargs\n",
        "):\n",
        "  \"\"\"Make a TextWorldExpressEnv RL environment.\"\"\"\n",
        "\n",
        "  # initialize game generator\n",
        "  env = TextWorldExpressEnv(envStepLimit=step_limit)\n",
        "\n",
        "  # set game default params and update with kwargs\n",
        "  game_params = {\n",
        "      \"numLocations\": numLocations,\n",
        "      \"includeDoors\": includeDoors,\n",
        "      \"numDistractorItems\": numDistractorItems,\n",
        "      \"numIngredients\": numIngredients,\n",
        "  }\n",
        "  game_params = \",\".join([f\"{k}={v}\" for k, v in game_params.items()])\n",
        "#\n",
        "  # load game\n",
        "  env.load(\n",
        "      gameName=game_name,\n",
        "      gameParams=game_params,\n",
        "  )\n",
        "\n",
        "  return env\n",
        "\n",
        "# test\n",
        "env = make_env()\n",
        "obs, info = env.reset(seed=123456, gameFold=\"train\")\n",
        "\n",
        "for key, value in info.items():\n",
        "  print(f\"**{key}**: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVVgqBzHO3_Q"
      },
      "source": [
        "## Collect expert dataset for SFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbPEUr85PbWf",
        "outputId": "3b83c969-72d2-41a7-f954-322fcb2ce5a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['look around',\n",
              " 'take cookbook',\n",
              " 'read cookbook',\n",
              " 'open cutlery drawer',\n",
              " 'take knife',\n",
              " 'take red apple',\n",
              " 'open fridge',\n",
              " 'take yellow bell pepper',\n",
              " 'close fridge',\n",
              " 'chop yellow bell pepper',\n",
              " 'slice red apple',\n",
              " 'cook yellow bell pepper in stove',\n",
              " 'prepare meal',\n",
              " 'eat meal']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "obs, info = env.reset(\n",
        "  seed=123456, gameFold=\"train\", generateGoldPath=True\n",
        ")\n",
        "env.getGoldActionSequence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viKzO3GWfClp"
      },
      "source": [
        "Collect random experience for supervised fine-tuning using the generate gold path function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e722ae09a5a14157970c46f2e3b08391",
            "cdf0df1095374a90acd29e4f255ab930",
            "53980ea5405b408fa1e5f505fd28fa46",
            "8e1a30b21ae441789ab3651f5dc9d163",
            "7544b9820b8b4ef39bb9fbadbe1bdf4e",
            "0dcd3b799bb14c7dbb8c0c0cc489df83",
            "1d2e7a6cada54851a280cd27485baf47",
            "22bc819a91184af98a891de3ae06dd76",
            "dd7212aed09346ba846031cd53195fe2",
            "76f65f3cd64843b99042dca78fe938d1",
            "f4503c9de9584500bd6aa15f579bfbfb"
          ]
        },
        "id": "RhAS-6gTfwO8",
        "outputId": "232a57b6-5090-47f6-8e09-e7526770f19f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/900 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e722ae09a5a14157970c46f2e3b08391"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "INSTRUCTION_TEMPLATE = (\n",
        "  \"# TASK: {}\\n\"\n",
        "  \"Choose exactly one valid action.\"\n",
        ")\n",
        "\n",
        "GAME_MSG_TEMPLATE = \"\\n# STEP {} \\nObs: {}\"\n",
        "\n",
        "def expert_rollout(\n",
        "  env, max_history=0, seed=None, gameFold=\"train\"\n",
        ") -> list[dict]:\n",
        "  \"Rollout a game using the provided model.\"\n",
        "\n",
        "  # reset the environment and obtain expert (goldPath) sequence\n",
        "  obs, info = env.reset(seed=seed, gameFold=gameFold, generateGoldPath=True)\n",
        "  expert_path = env.getGoldActionSequence()\n",
        "\n",
        "  # obtain instruction from game\n",
        "  instr = INSTRUCTION_TEMPLATE.format(info[\"taskDescription\"])\n",
        "\n",
        "  # create buffers\n",
        "  messages = [instr]\n",
        "\n",
        "  # rollout loop\n",
        "  done = False\n",
        "  step = 0\n",
        "  transitions = []\n",
        "  while not done:\n",
        "    # format observation\n",
        "    game_msg = GAME_MSG_TEMPLATE.format(step, obs) # , valid)\n",
        "\n",
        "    # make prompt for the LLM\n",
        "    valid = \", \".join(info[\"validActions\"])\n",
        "    question = f\"\\nValid actions: {valid}\\nAction: \"\n",
        "    prompt_msgs = messages[-max_history:] + [game_msg] + [question]\n",
        "    if step > max_history:\n",
        "        prompt_msgs.insert(0, messages[0])\n",
        "    prompt = ''.join(prompt_msgs)\n",
        "\n",
        "    # get expert action\n",
        "    expert_action = expert_path.pop(0)\n",
        "\n",
        "    # step the environment\n",
        "    obs, reward, done, info = env.step(expert_action)\n",
        "\n",
        "    # add to transitions\n",
        "    transitions.append(\n",
        "      {\"prompt\": prompt, \"action\": expert_action, \"reward\": reward, \"done\": done}\n",
        "    )\n",
        "    messages.append(game_msg + \"\\nAction: \" + expert_action)\n",
        "    step += 1\n",
        "\n",
        "  return transitions\n",
        "\n",
        "\n",
        "# outer loop / create dataset\n",
        "rollouts = []\n",
        "num_rollouts = 900\n",
        "max_history = 11\n",
        "transitions = []\n",
        "for i in tqdm(range(num_rollouts)):\n",
        "  transitions = expert_rollout(env, seed=123456 * i, max_history=max_history)\n",
        "  rollouts.extend(transitions)\n",
        "dataset = Dataset.from_list(rollouts)\n",
        "\n",
        "# get train/eval splits\n",
        "num_prompts = len(rollouts)\n",
        "num_train = 500\n",
        "num_eval = 100\n",
        "sample = np.random.choice(num_prompts, num_train + num_eval, replace=False)\n",
        "train_sample = sample[:num_train]\n",
        "eval_sample = sample[num_train:]\n",
        "train_dataset = dataset.select(train_sample)\n",
        "eval_dataset = dataset.select(eval_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "4cL3VbJY9FXi",
        "outputId": "5f7af96b-da6b-49ed-bab8-0380384a2f4e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Prompt"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# TASK: You are hungry! Let's cook a delicious meal. Check the cookbook in the kitchen for the recipe. Once done, enjoy your meal!\n",
            "Choose exactly one valid action.\n",
            "# STEP 0 \n",
            "Obs: You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter that has a knife, a banana, and a cookbook on it. In one part of the room you see a dining table, that has nothing on it. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \n",
            "To the North you see a closed patio door. To the South you see a closed plain door. \n",
            "Action: look around\n",
            "# STEP 1 \n",
            "Obs: You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter that has a knife, a banana, and a cookbook on it. In one part of the room you see a dining table, that has nothing on it. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \n",
            "To the North you see a closed patio door. To the South you see a closed plain door. \n",
            "Action: take cookbook\n",
            "# STEP 2 \n",
            "Obs: You take the cookbook.\n",
            "Action: read cookbook\n",
            "# STEP 3 \n",
            "Obs: Gather all following ingredients and follow the directions to prepare this tasty meal.\n",
            "\n",
            "Ingredients:\n",
            "  banana\n",
            "  chicken wing\n",
            "\n",
            "Directions:\n",
            "  slice the banana\n",
            "  grill the banana\n",
            "  grill the chicken wing\n",
            "  prepare meal\n",
            "\n",
            "\n",
            "Action: take knife\n",
            "# STEP 4 \n",
            "Obs: You take the knife.\n",
            "Valid actions: put cookbook in dining chair, take banana, examine counter, open fridge, close door to north, examine dishwasher, move south, examine cookbook, examine trash can, put knife in dining table, examine stove, put knife in dining chair, examine banana, move north, look around, examine knife, put knife in counter, open dishwasher, open cutlery drawer, open door to south, examine fridge, open door to north, examine oven, examine dining table, inventory, read cookbook, open trash can, examine dining chair, examine cutlery drawer, close door to south, put cookbook in counter, put cookbook in dining table\n",
            "Action: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Expert action"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "take banana\n"
          ]
        }
      ],
      "source": [
        "printmd(\"### Prompt\")\n",
        "print(train_dataset[\"prompt\"][1])\n",
        "printmd(\"### Expert action\")\n",
        "print(train_dataset[\"action\"][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W8c0ZtMsjz2"
      },
      "source": [
        "## Supervised Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrQyOQytaQ4G",
        "outputId": "ead32383-51bc-4990-d949-d276f790953f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model_name = \"Rocketknight1/falcon-rw-1b\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
        "tokenizer.padding_side = 'left'\n",
        "tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f170cd8066fa4b228a316d2eae351b93",
            "7d268349a55e48b6821aa4a50b9d3ceb",
            "679a338a56d44adfb6af1ebc2b1f043d",
            "4994efa3a0e24a84bbb0a03b67a35c18",
            "5cd71e1265704657a20d8e5d922d0c4e",
            "95490ad4aa984102b1419310bd6fa06e",
            "fc01a8795f4a48999603c91d4f304649",
            "b5adc966f91a49f8a750d2324f836d4e",
            "5aa7604932224ff79e16a099c95bef35",
            "dcef2d01eac54be68a6b5d0b4e2de535",
            "50f24095301f446888527def2caf97dd"
          ]
        },
        "id": "H79ao12bHfD-",
        "outputId": "24984e4a-9a5c-49ab-d5ee-0cf756632874"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f170cd8066fa4b228a316d2eae351b93"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "MAX_SEQ_LEN = 2048  # ensures things will run on the free T4 GPU\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['prompt']\n",
        "    targets = examples['action']\n",
        "    targets = [x + tokenizer.eos_token for x in targets]\n",
        "\n",
        "    tok_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=MAX_SEQ_LEN,\n",
        "        truncation=True,\n",
        "    )\n",
        "    tok_targets = tokenizer(targets)\n",
        "    combined_tokens = [\n",
        "        x + y\n",
        "        for x, y in zip(tok_inputs.input_ids, tok_targets.input_ids)\n",
        "    ]\n",
        "    attention_mask = [\n",
        "        x + y\n",
        "        for x, y in zip(tok_inputs.attention_mask, tok_targets.attention_mask)\n",
        "    ]\n",
        "    target_lengths = [len(x) for x in tok_targets['input_ids']]\n",
        "\n",
        "    # make input/target pairs for next token prediction\n",
        "    input_ids = [x[:-1] for x in combined_tokens]\n",
        "    labels = [x[1:] for x in combined_tokens]\n",
        "\n",
        "    # for any other part of examples, include\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'labels': labels,\n",
        "        'target_lengths': target_lengths,\n",
        "        'reward': examples['reward'],\n",
        "    }\n",
        "\n",
        "tokenized_datasets = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "# Convert lists to tensors using DataCollator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
        "\n",
        "# Data loader\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    tokenized_datasets,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH9_VHtsa5S6",
        "outputId": "b1187985-58bf-49a8-f9e6-5e3722072809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward: torch.Size([1])\n",
            "input_ids: torch.Size([1, 724])\n",
            "target_lengths: torch.Size([1])\n",
            "attention_mask: torch.Size([1, 724])\n",
            "labels: torch.Size([1, 724])\n"
          ]
        }
      ],
      "source": [
        "for k, v in next(iter(train_dataloader)).items():\n",
        "  print(f\"{k}: {v.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqNqZ6jVKxgU",
        "outputId": "6bc3254b-fe0a-40bd-9531-705a9b021cea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,145,728 || all params: 1,314,770,944 || trainable%: 0.2393\n"
          ]
        }
      ],
      "source": [
        "import bitsandbytes as bnb\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
        "\n",
        "# dtype = torch.bfloat16. # ~ for some reason is super slow in T4 Colab GPU\n",
        "dtype = torch.float16\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_compute_dtype=dtype,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    torch_dtype=dtype,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_cache = False\n",
        ")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "  task_type=TaskType.CAUSAL_LM,\n",
        "  r=4,\n",
        "  lora_alpha=32,\n",
        "  target_modules=\"all-linear\",\n",
        "  lora_dropout=0.25,\n",
        "  bias=\"none\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2yOi9RoK0rE"
      },
      "source": [
        "Make an initial test of the model generation capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "4X8SFvwKK454",
        "outputId": "777b6aa5-7c24-4e95-f492-8de8f54b0b5d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Prompt"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# TASK: You are hungry! Let's cook a delicious meal. Check the cookbook in the kitchen for the recipe. Once done, enjoy your meal!\n",
            "Choose exactly one valid action.\n",
            "# STEP 0 \n",
            "Obs: You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter that has a knife, and a cookbook on it. In one part of the room you see a dining table, that has nothing on it. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \n",
            "To the North you see a closed sliding patio door. To the East you see a closed plain door. \n",
            "Action: look around\n",
            "# STEP 1 \n",
            "Obs: You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter that has a knife, and a cookbook on it. In one part of the room you see a dining table, that has nothing on it. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \n",
            "To the North you see a closed sliding patio door. To the East you see a closed plain door. \n",
            "Action: take cookbook\n",
            "# STEP 2 \n",
            "Obs: You take the cookbook.\n",
            "Valid actions: take knife, move north, open door to east, examine cutlery drawer, inventory, examine knife, examine cookbook, examine oven, examine fridge, examine dining chair, put cookbook in dining table, read cookbook, close door to north, examine trash can, open trash can, put cookbook in counter, open door to north, open cutlery drawer, examine counter, move east, open fridge, examine dishwasher, examine stove, examine dining table, put cookbook in dining chair, look around, open dishwasher, close door to east\n",
            "Action: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Answer without finetuning"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Valid actions: look around, look at knife, look at counter,\n"
          ]
        }
      ],
      "source": [
        "def generate_answer(prompt, model, tokenizer, sample=False):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    tokens = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "    tokens = tokens.to(model.device)\n",
        "    input_len = tokens.input_ids.shape[1]\n",
        "\n",
        "    # Generate the output\n",
        "    output = model.generate(\n",
        "        input_ids=tokens.input_ids,\n",
        "        attention_mask=tokens.attention_mask,\n",
        "        do_sample=sample,\n",
        "        num_return_sequences=1,\n",
        "        max_new_tokens=15,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decode the generated answer\n",
        "    answer = output[0][input_len:]\n",
        "    answer = tokenizer.decode(answer, skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "prompt = eval_dataset[1]['prompt']\n",
        "printmd(\"### Prompt\")\n",
        "print(prompt)\n",
        "\n",
        "answer = generate_answer(prompt, model, tokenizer)\n",
        "printmd(\"### Answer without finetuning\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyhqwaTZpZvN"
      },
      "source": [
        "Next cell takes ~10 minutes for 3 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154,
          "referenced_widgets": [
            "d2701380c28242f6912c6e9de46464cd",
            "05399666d71a4e9999e026d0930a3d96",
            "285ca7055af4423a92bfcb5f0f9ddc7a",
            "b36d1b6b46d6496f8ace404df5846ba9",
            "8e8f3846c9844a4cbb33b6c9cfb59382",
            "8cb8a24addeb4b928aa08ef55a700698",
            "c506beec4d2a43d0a95ab39df2f936bd",
            "270e08503afe4e6bbf7402a78ab29c97",
            "4ad319cccf334c20a963a5074bf031fa",
            "0c7c0730b21d4e9ab76d8fdd6aa2ce85",
            "0f4bc6cb808341c9aea5065accb66dc9"
          ]
        },
        "id": "G5egCT0iL8Uw",
        "outputId": "236b2ca3-3047-47da-8dba-b42d3c8ac78c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Epoch 0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2701380c28242f6912c6e9de46464cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Loss**: 0.9739"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "265"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from accelerate import Accelerator\n",
        "import gc\n",
        "\n",
        "# Num epochs\n",
        "num_epochs = 1\n",
        "gradient_accumulation_steps = 8\n",
        "\n",
        "# Optimizer (8 bit variant)\n",
        "opt = bnb.optim.PagedAdamW8bit(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "\n",
        "# Scheduler\n",
        "num_training_steps = len(train_dataloader) * num_epochs  # 3 epochs\n",
        "scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=opt,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# Loss fn\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# Setup the accelerator to handle devices and gradient accumulation\n",
        "accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "model, opt, train_dataloader, scheduler = accelerator.prepare(\n",
        "  model, opt, train_dataloader, scheduler\n",
        ")\n",
        "\n",
        "# Training step function\n",
        "def train_step(batch, model, opt, scheduler, accelerator):\n",
        "    with accelerator.accumulate(model):\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask']\n",
        "        )\n",
        "\n",
        "        # Compute cross-entropy loss. Only count losses for predicting\n",
        "        # The completion, not the original text.\n",
        "        losses = []\n",
        "        batch_size = len(batch['input_ids'])\n",
        "        for b in range(batch_size):\n",
        "            L = batch['target_lengths'][b]\n",
        "            loss = loss_fn(outputs.logits[b][-L:], batch['labels'][b][-L:])\n",
        "            losses.append(loss)\n",
        "        loss = sum(losses) / len(losses)\n",
        "\n",
        "        # Backward pass\n",
        "        accelerator.backward(loss)\n",
        "        opt.step()\n",
        "        scheduler.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    printmd(f\"### Epoch {epoch}\")\n",
        "\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "\n",
        "    for batch in tqdm(train_dataloader, total=len(train_dataloader)):\n",
        "        loss = train_step(batch, model, opt, scheduler, accelerator)\n",
        "        epoch_losses.append(loss)\n",
        "\n",
        "    printmd(f\"**Loss**: {np.mean(epoch_losses):.4f}\")\n",
        "\n",
        "\n",
        "print(\"Training completed\")\n",
        "model.save_pretrained(\"ckpt_sft.pt\")\n",
        "\n",
        "# Clean up\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "dxJ3HVd8hNSo",
        "outputId": "3b259d93-e2d5-42e3-eeef-e79a71102b13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Prompt"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# TASK: You are hungry! Let's cook a delicious meal. Check the cookbook in the kitchen for the recipe. Once done, enjoy your meal!\n",
            "Choose exactly one valid action.\n",
            "# STEP 4 \n",
            "Obs: You open the cutlery drawer. The cutlery drawer contains a knife.\n",
            "Action: take knife\n",
            "# STEP 5 \n",
            "Obs: You take the knife.\n",
            "Action: take red apple\n",
            "# STEP 6 \n",
            "Obs: You take the red apple.\n",
            "Action: open fridge\n",
            "# STEP 7 \n",
            "Obs: You open the fridge. It's empty inside.\n",
            "Action: close fridge\n",
            "# STEP 8 \n",
            "Obs: You close the fridge.\n",
            "Action: open kitchen cupboard\n",
            "# STEP 9 \n",
            "Obs: You open the kitchen cupboard. It's empty inside.\n",
            "Action: close kitchen cupboard\n",
            "# STEP 10 \n",
            "Obs: You close the kitchen cupboard.\n",
            "Action: open door to north\n",
            "# STEP 11 \n",
            "Obs: You open the plain door, revealing the pantry. \n",
            "Action: move north\n",
            "# STEP 12 \n",
            "Obs: You are in the pantry. In one part of the room you see a folding chair, that has nothing on it. There is also a shelf that has some salt on it. \n",
            "Through an open plain door, to the South you see the kitchen. \n",
            "Action: move south\n",
            "# STEP 13 \n",
            "Obs: You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also an open cutlery drawer, that is empty. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \n",
            "Through an open plain door, to the North you see the pantry. To the South you see a closed sliding patio door. \n",
            "Action: open door to south\n",
            "# STEP 14 \n",
            "Obs: You open the sliding patio door, revealing the backyard. \n",
            "Action: move south\n",
            "# STEP 15 \n",
            "Obs: You are in the backyard. In one part of the room you see a barbeque. There is also a workbench, that has nothing on it. You also see a patio chair, that has nothing on it. In another part of the room you see a patio table, that has nothing on it. In one part of the room you see a clothes line, that has nothing on it. There is also a garden that has a raw purple potato, and a banana on it. \n",
            "Through an open sliding patio door, to the North you see the kitchen. \n",
            "Valid actions: put knife in patio table, chop red apple, slice red apple, open door to north, examine red apple, put red apple in workbench, put cookbook in garden, close door to north, examine barbeque, put red apple in garden, put knife in workbench, examine clothes line, read cookbook, examine banana, put red apple in clothes line, dice red apple, examine knife, put cookbook in patio chair, examine patio chair, take banana, put red apple in patio table, examine patio table, put knife in clothes line, eat red apple, cook red apple in barbeque, put knife in garden, look around, examine cookbook, examine purple potato, put cookbook in clothes line, put cookbook in patio table, put knife in patio chair, examine workbench, inventory, put cookbook in workbench, put red apple in patio chair, examine garden, take purple potato, move north\n",
            "Action: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Answer with finetuning"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "move south\n"
          ]
        }
      ],
      "source": [
        "prompt = train_dataset[0]['prompt']\n",
        "printmd(\"### Prompt\")\n",
        "print(prompt)\n",
        "\n",
        "answer = generate_answer(prompt, model, tokenizer)\n",
        "printmd(\"### Answer with finetuning\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Co28tP1pGnm"
      },
      "source": [
        "# ^ Training loop working!!!\n",
        "1B LLM generated sensible responses!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aCXhuSfbC2X"
      },
      "source": [
        "# Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa9Iuu2GU52Z"
      },
      "source": [
        "<a name=\"methodology\"></a>\n",
        "# Methodology\n",
        "\n",
        "\n",
        "### This section's content:\n",
        "\n",
        "1. [Initial evaluation of pure LLM agents](#initial-eval)\n",
        "  - 1.1. [Prompt design and test](#prompt-design)\n",
        "  - 1.2. [Rollout evaluation](#rollout-eval)\n",
        "2. [Improvement via RL](#rl-improvement)\n",
        "  - 2.1 [Policy/Value networks and LORA](#arch)\n",
        "  - 2.2 [Gym wrapper](#gym-wrapper)\n",
        "  - 2.3 [PPO Training](#ppo)\n",
        "3. Combining RL with RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWXsiZ5freTG"
      },
      "source": [
        "<a name=\"initial-eval\"></a>\n",
        "## 1. Initial evaluation of pure LLM agents\n",
        "\n",
        "\n",
        "<a name=\"prompt-design\"></a>\n",
        "### 1.1 Simple prompt design\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgEIP1Nov91A"
      },
      "source": [
        "THIS SECTION IS DEPRECATED.\n",
        "PROMPT IS ABOVE\n",
        "\n",
        "<!-- Now test using an LLM to select one action. We need to **create a prompt** do do so. The ingredients for the prompt are.\n",
        "1. The overall instruction.\n",
        "2. A function `get_state` to transform the agent's past history an actions into a textual representation. (We will initially set this to concatenation)\n",
        "3. A function `make_prompt` that takes the state and the environment's allowed actions to create a prompt for the LLM.\n",
        "\n",
        "We provide an initial basic implementation of these functions that will be refined throughout the tutorial. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKuDbcTkF0V3"
      },
      "source": [
        "<a name=\"rollout-eval\"><a/>\n",
        "### 2.2 Rollout Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb1soDDJLsot"
      },
      "source": [
        "We will write a quick script for evaluating the models in a number of games"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18VyAUpcK2xY"
      },
      "source": [
        "### 2. RL Improvement\n",
        "<a name=\"rl-improvement\"></a>\n",
        "\n",
        "We will now use PPO and LORA to optimize the underperforming LLM.\n",
        "\n",
        "<!-- We will need the following ingredients:\n",
        "\n",
        "1. A definition of policy and value networks.\n",
        "2. A wrapper of the TextWorld environment for compatibility with the `gynnasium` and `stable-baselines` framewrok. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-6g1GyptIiL"
      },
      "source": [
        "### Policy and Value Networks and LORA\n",
        "<a name=\"arch\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1zxkFm0NvWG"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "  \"\"\"Agent class modeled after transformers.AutoModelForCausalLMWithValueHead\"\"\"\n",
        "  transformers_parent_class = AutoModelForCausalLM\n",
        "\n",
        "  def __init__(self, model, tokenizer):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    # value function related stuff\n",
        "    self.hidden_size = self.model.config.hidden_size\n",
        "\n",
        "    # define and initialize critic head\n",
        "    self.v_head = nn.Linear(\n",
        "      self.hidden_size, 1, dtype=self.model.dtype\n",
        "    ).to(self.model.device)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      output_hidden_states=True,\n",
        "    )\n",
        "\n",
        "    # here's a small modification from PPO for RLHF since we want the\n",
        "    # last_token_hidden_state not the last_hidden_state for all tokens\n",
        "    last_token_hidden_state = outputs.hidden_states[-1]\n",
        "    lm_logits = outputs.logits\n",
        "    loss = outputs.loss\n",
        "    value = self.v_head(last_token_hidden_state).squeeze(-1)\n",
        "\n",
        "    return (lm_logits, loss, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCN4G9UX43N2"
      },
      "source": [
        "We will now perform the main RL training loop. The expected running time is about 15 mins. Consider that this time is for demonstration, as more training is needed for good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "yRxs3A3LncZ9",
        "outputId": "7179562b-da22-4b52-a912-96a0a4ab1100"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-f03e7f2582a8>, line 178)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f03e7f2582a8>\"\u001b[0;36m, line \u001b[0;32m178\u001b[0m\n\u001b[0;31m    full_log_prob =\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#  epoch breakdown ~  5 min per epoch. 20 epochs ~ 100 minutes\n",
        "from argparse import Namespace\n",
        "import ipdb\n",
        "\n",
        "# Hyperparameters\n",
        "cfg = {\n",
        "  # Optimization parameters\n",
        "  \"epochs\": 20,\n",
        "  \"lr\": 1.41e-5,  # from hugging face PPO trainer\n",
        "  \"clip_grad_norm\": None,\n",
        "  \"clip_value\": 0.2,\n",
        "  # PPO hyperparams\n",
        "  \"gamma\": 1.0,\n",
        "  \"clip_ratio\": 0.2,  # clip ratio\n",
        "  \"wd\": 1e-8,\n",
        "  \"whiten_rewards\": False,\n",
        "  \"entropy_coef\": 0.0,\n",
        "  \"vf_coef\": 0.1,\n",
        "  \"gae_lambda\": 0.95,\n",
        "  \"kl_coef\": 0.2,\n",
        "  # Data colleciton loop setup\n",
        "  \"num_rollouts\": 20,\n",
        "  \"max_history\": STEP_LIMIT - 1,\n",
        "  \"num_training_samples\": 320,\n",
        "  # Optimization params\n",
        "  \"batch_size\" : 1,\n",
        "  \"gradient_accumulation_steps\": 8,\n",
        "  \"inner_optimization_epochs\": 1,\n",
        "}\n",
        "cfg = Namespace(**cfg)\n",
        "\n",
        "\n",
        "# =============================\n",
        "# ==== Auxiliary functions ====\n",
        "# =============================\n",
        "\n",
        "def logprobs_from_logits(logits, labels):\n",
        "  logp = F.log_softmax(logits, dim=-1)\n",
        "  logpy = torch.gather(logp, -1, labels.unsqueeze(-1)).squeeze(-1)\n",
        "  return logpy\n",
        "\n",
        "\n",
        "def scale_rewards(rewards, whiten_rewards=True):\n",
        "  if whiten_rewards:\n",
        "      r_mu, r_sig = np.mean(rewards), np.std(rewards) + 1e-2\n",
        "      scaled_rewards = [(r - r_mu) / r_sig for r in rewards]\n",
        "  else:\n",
        "      scaled_rewards = rewards\n",
        "  return scaled_rewards\n",
        "\n",
        "# we will define model_rollout, rollout_step, training_step, evaluation_step\n",
        "\n",
        "def model_rollout(\n",
        "  env,\n",
        "  model,\n",
        "  tokenizer,\n",
        "  max_history=0,\n",
        "  seed=None,\n",
        "  gameFold=\"test\",\n",
        "  # eps: float = 0.0,\n",
        "  sample=False,\n",
        ") -> list[dict]:\n",
        "  \"Rollout a game using the provided model.\"\n",
        "\n",
        "  # reset the environment and obtain expert (goldPath) sequence\n",
        "  obs, info = env.reset(seed=seed, gameFold=gameFold)\n",
        "\n",
        "  # obtain instruction from game\n",
        "  instr = INSTRUCTION_TEMPLATE.format(info[\"taskDescription\"])\n",
        "\n",
        "  # create buffers\n",
        "  messages = [instr]\n",
        "\n",
        "  # rollout loop\n",
        "  done = False\n",
        "  step = 0\n",
        "  transitions = []\n",
        "  while not done:\n",
        "    # format observation\n",
        "    game_msg = GAME_MSG_TEMPLATE.format(step, obs)\n",
        "\n",
        "    # make prompt for the LLM\n",
        "    valid = \", \".join(info[\"validActions\"])\n",
        "    question = f\"\\nValid actions: {valid}\\nAction: \"\n",
        "    prompt_msgs = messages[-max_history:] + [game_msg] + [question]\n",
        "    if step > max_history:\n",
        "        prompt_msgs.insert(0, messages[0])\n",
        "    prompt = ''.join(prompt_msgs)\n",
        "\n",
        "    # Get answer from LLM\n",
        "    action = generate_answer(prompt, model, tokenizer, sample=sample)\n",
        "\n",
        "    # step the environment\n",
        "    obs, reward, done, info = env.step(action)\n",
        "\n",
        "    # add to transitions\n",
        "    transitions.append(\n",
        "      {\"prompt\": prompt, \"action\": action, \"reward\": reward, \"done\": done}\n",
        "    )\n",
        "    messages.append(game_msg + \"\\nAction: \" + action)\n",
        "    step += 1\n",
        "\n",
        "  return transitions\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def rollout_step(env, agent, tokenizer, ref_model, cfg):\n",
        "  agent.eval()\n",
        "  transitions = []\n",
        "  returns = []\n",
        "  for i in tqdm(range(cfg.num_rollouts)):\n",
        "    # == Collect data ===\n",
        "    out = model_rollout(\n",
        "      env,\n",
        "      agent.model,\n",
        "      tokenizer,\n",
        "      max_history=cfg.max_history,\n",
        "      gameFold=\"train\",\n",
        "      sample=True,\n",
        "    )\n",
        "    transitions.extend(out)\n",
        "    returns.append(sum([t[\"reward\"] for t in out]))\n",
        "\n",
        "  # Precompute scaling contants\n",
        "  r_mu, r_sig = 0, 1\n",
        "  rewards = [t[\"reward\"] for t in transitions]\n",
        "  if cfg.whiten_rewards:\n",
        "    r_mu, r_sig = np.mean(rewards), np.std(rewards) + 1e-2\n",
        "\n",
        "  # Compute discounted returns, values, and advantages, etc.\n",
        "  disc_return = 0\n",
        "  next_value = 0\n",
        "  advantage = 0\n",
        "  buffer = []\n",
        "  for i in reversed(range(len(transitions))):\n",
        "    tr = transitions[i]\n",
        "\n",
        "    # compute discounted return\n",
        "    scaled_reward = (tr['reward'] - r_mu) / r_sig\n",
        "    disc_return = scaled_reward + cfg.gamma * disc_return * (1 - tr[\"done\"])\n",
        "\n",
        "    # Prepare tokens for model evaluation, needed for values and log probs\n",
        "    prompt_tokens = tokenizer(\n",
        "      tr[\"prompt\"],\n",
        "      return_tensors=\"pt\",\n",
        "      truncation=True,\n",
        "      max_length=MAX_SEQ_LEN\n",
        "    ).input_ids\n",
        "    target_tokens = tokenizer(\n",
        "        tr[\"action\"] + tokenizer.eos_token, return_tensors=\"pt\"\n",
        "    ).input_ids\n",
        "    cat = torch.cat([prompt_tokens, target_tokens], dim=-1)\n",
        "    input_ids = cat[0, :-1]\n",
        "    labels = cat[0, 1:]\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    target_length = len(target_tokens[0])\n",
        "\n",
        "    # == Obtain value, log probs and advantage ==\n",
        "\n",
        "    # Prepare inputs\n",
        "    input_ids = input_ids.to(accelerator.device)\n",
        "    attention_mask = attention_mask.to(accelerator.device)\n",
        "\n",
        "    # Obtain value\n",
        "    curr_value = value if i < len(transitions) - 1 else 0\n",
        "    logits, _, value = agent(input_ids, attention_mask)\n",
        "\n",
        "    # Value output is for each token, take value at last token of prompt\n",
        "    # before the completion/action by offseting by target length\n",
        "    value = value[:, -target_length].item()\n",
        "\n",
        "    # Obtain advantage with LamGAE\n",
        "    next_value = next_value * (1 - tr[\"done\"])\n",
        "    delta = scaled_reward + cfg.gamma * next_value - value\n",
        "    advantage = delta + cfg.gamma * cfg.gae_lambda * advantage\n",
        "\n",
        "    # Obtain log prob of each token of the response\n",
        "    full_log_prob =\n",
        "    log_prob = logprobs_from_logits(\n",
        "      logits[:, -target_length:],\n",
        "      labels[None, -target_length:].to(accelerator.device)\n",
        "    ).sum()\n",
        "\n",
        "    # Obtain reference logits and compute KL penalty\n",
        "    ref_logits = ref_model(input_ids, attention_mask).logits\n",
        "    ref_log_prob = logprobs_from_logits(\n",
        "      ref_logits[:, -target_length:],\n",
        "      labels[None, -target_length:].to(accelerator.device)\n",
        "    ).sum()\n",
        "\n",
        "\n",
        "\n",
        "    buffer.append(\n",
        "      {\n",
        "        'input_ids': input_ids,\n",
        "        'labels': labels,\n",
        "        'attention_mask': attention_mask,\n",
        "        'target_lengths': target_length,\n",
        "        'log_prob': log_prob,\n",
        "        'value': value,\n",
        "        'scaled_reward': scaled_reward,\n",
        "        'advantage': advantage,\n",
        "        'disc_return': disc_return,\n",
        "      }\n",
        "    )\n",
        "\n",
        "  # Make buffer for training\n",
        "  buffer = Dataset.from_list(buffer)\n",
        "  subsample = min(len(buffer), cfg.num_training_samples)\n",
        "  buffer = buffer.shuffle().select(range(subsample))\n",
        "  loader = DataLoader(\n",
        "    buffer, batch_size=cfg.batch_size, collate_fn=data_collator, shuffle=True\n",
        "  )\n",
        "\n",
        "  return loader, returns\n",
        "\n",
        "\n",
        "def training_step(agent, loader, opt, accelerator, cfg):\n",
        "  # Prepare model, loader, etc with accelerate\n",
        "  agent.train()\n",
        "  agent, loader, opt = accelerator.prepare(agent, loader, opt)\n",
        "\n",
        "  # Buffers for loss from each batch\n",
        "  policy_losses = []\n",
        "  value_losses = []\n",
        "  entropy_losses = []\n",
        "  kl_losses = []\n",
        "  total_losses = []\n",
        "\n",
        "  # Get dtype from model\n",
        "  dtype = agent.model.dtype\n",
        "\n",
        "  for epoch in tqdm(range(cfg.inner_optimization_epochs)):\n",
        "    for batch in loader:\n",
        "      with accelerator.accumulate(agent):\n",
        "        new_value = []\n",
        "        entropy_loss = []\n",
        "        policy_loss = []\n",
        "        kl_loss = []\n",
        "\n",
        "        for j in range(cfg.batch_size):\n",
        "          # Compute state value from prompt + action tokens\n",
        "          target_length = batch.target_lengths[j]\n",
        "          logits, _, value = agent(\n",
        "            batch.input_ids[j, None], batch.attention_mask[j, None]\n",
        "          )\n",
        "          logits = logits[:, -target_length:]\n",
        "          labels = batch.labels[j, None, -target_length:]\n",
        "          new_value.append(value[:, -target_length])\n",
        "\n",
        "          # Entropy loss\n",
        "          pd = F.softmax(logits, -1)\n",
        "          neg_ent = torch.logsumexp(logits, -1) - (pd * logits).sum(-1)\n",
        "          entropy_loss.append(neg_ent.mean())\n",
        "\n",
        "          # Policy loss\n",
        "          new_log_prob = logprobs_from_logits(logits, labels).sum()\n",
        "          old_log_prob = batch.log_prob[j].to(dtype)\n",
        "          adv = batch.advantage.to(dtype).unsqueeze(-1)\n",
        "          prob_ratio = (new_log_prob - old_log_prob).clamp(-10, 10).exp()\n",
        "          surr1 = - prob_ratio * adv\n",
        "          surr2 = - prob_ratio.clamp(1 - cfg.clip_ratio, 1 + cfg.clip_ratio) * adv\n",
        "          policy_loss.append(torch.max(surr1, surr2).mean())\n",
        "\n",
        "          # KL loss\n",
        "          ipdb.set_trace\n",
        "          kl_loss.append(0.5 * (new_log_prob - old_log_prob)**2)\n",
        "\n",
        "        # Critic loss / as in PPO hugging face\n",
        "        disc_return = batch.disc_return.to(dtype)\n",
        "        old_value = batch.value.to(dtype)\n",
        "\n",
        "        new_value = torch.cat(new_value)\n",
        "        new_value_clipped = new_value.clamp(\n",
        "          old_value - cfg.clip_value,\n",
        "          old_value + cfg.clip_value,\n",
        "        )\n",
        "        value_loss_1 = (new_value - disc_return) ** 2\n",
        "        value_loss_2 = (new_value_clipped - disc_return) ** 2\n",
        "        value_loss = 0.5 * torch.max(value_loss_1, value_loss_2).mean()\n",
        "\n",
        "        # Total loss\n",
        "        kl_loss = sum(kl_loss) / cfg.batch_size\n",
        "        policy_loss = sum(policy_loss) / cfg.batch_size\n",
        "        entropy_loss = sum(entropy_loss) / cfg.batch_size\n",
        "        loss = (\n",
        "          policy_loss\n",
        "          + cfg.vf_coef * value_loss\n",
        "          + cfg.entropy_coef * entropy_loss\n",
        "          + cfg.kl_coef * kl_loss\n",
        "        )\n",
        "\n",
        "        # Optimize the model\n",
        "        accelerator.backward(loss)\n",
        "        if cfg.clip_grad_norm is not None:\n",
        "          accelerator.clip_grad_norm_(agent.parameters(), cfg.clip_grad_norm)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Update metrics\n",
        "        policy_losses.append(policy_loss.item())\n",
        "        value_losses.append(value_loss.item())\n",
        "        entropy_losses.append(entropy_loss.mean().item())\n",
        "        kl_losses.append(kl_loss.mean().item())\n",
        "        total_losses.append(loss.item())\n",
        "\n",
        "  return {\n",
        "    \"policy_loss\": policy_losses,\n",
        "    \"value_loss\": value_losses,\n",
        "    \"entropy_loss\": entropy_losses,\n",
        "    \"kl_loss\": kl_losses,\n",
        "    \"total_loss\" : total_losses\n",
        "  }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_step(agent, env, tokenizer, cfg):\n",
        "  returns = []\n",
        "  agent.eval()\n",
        "  for i in tqdm(range(cfg.num_rollouts)):\n",
        "    transitions = model_rollout(\n",
        "      env,\n",
        "      agent.model,\n",
        "      tokenizer,\n",
        "      max_history=cfg.max_history,\n",
        "      gameFold=\"test\",\n",
        "      sample=False\n",
        "    )\n",
        "    returns.append(sum([t[\"reward\"] for t in transitions]))\n",
        "\n",
        "  return returns\n",
        "\n",
        "# ===================================\n",
        "# ==== Minimal PPO training loop ====\n",
        "# ===================================\n",
        "\n",
        "# Agent\n",
        "agent = ActorCritic(model, tokenizer)\n",
        "\n",
        "# Environment\n",
        "env = make_env(step_limit=STEP_LIMIT)  # short games\n",
        "\n",
        "# Optimizer\n",
        "opt = bnb.optim.PagedAdamW8bit(agent.parameters(), lr=cfg.lr, weight_decay=cfg.wd)\n",
        "\n",
        "# Accelerator\n",
        "accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "\n",
        "# Clean memory before starting to reduce change of memory errors\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Best model checkpointing\n",
        "best_eval_return = -np.inf\n",
        "\n",
        "# Results\n",
        "eval_mean_returns = []\n",
        "train_mean_returns = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(cfg.epochs):\n",
        "  printmd(f\"### Epoch {epoch + 1}\")\n",
        "\n",
        "  print(f\"Collecting rollouts...\")\n",
        "  loader, train_returns = rollout_step(env, agent, tokenizer, cfg)\n",
        "\n",
        "  print(f\"Training...\")\n",
        "  losses = training_step(agent, loader, opt, accelerator, cfg)\n",
        "\n",
        "  print(f'Evaluating...')\n",
        "  eval_returns = eval_step(agent, env, tokenizer, cfg)\n",
        "\n",
        "  # Checkpoints\n",
        "  eval_mean_returns.append(np.mean(eval_returns))\n",
        "  train_mean_returns.append(np.mean(train_returns))\n",
        "\n",
        "  if eval_mean_returns[-1] > best_eval_return:\n",
        "    best_eval_return = eval_mean_returns[-1]\n",
        "    agent.model.save_pretrained(\"ckpt_best_rl.pt\")\n",
        "\n",
        "  # Print mebtrics\n",
        "  losses[\"mean_train_return\"] = train_mean_returns[-1]\n",
        "  losses[\"mean_eval_return\"] = eval_mean_returns[-1]\n",
        "  msg = ', '.join([f\"**{k}**: {np.mean(v):.3f}\" for k, v in losses.items()])\n",
        "  printmd(msg)\n",
        "\n",
        "# Clean up memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAU8LgMWq_wS"
      },
      "outputs": [],
      "source": [
        "# Eval best model\n",
        "best_model = AutoModelForCausalLM.from_pretrained(\"ckpt_best_rl.pt\")\n",
        "eval_returns = eval_step(best_model, env, tokenizer, cfg)\n",
        "print(np.mean(eval_returns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-g66RKyqoxK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'train': train_mean_returns, 'eval': eval_mean_returns})\n",
        "df.plot()\n",
        "plt.title('RL results')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sph9pXF-Rbk1"
      },
      "source": [
        "Now use a quick loop to evaluate the improved agent's performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8fp6ZxwjuwA"
      },
      "source": [
        "### 😡 Why is our eval policy doing so much worse than training time collection? Tried both `sample=True` and `sample=False`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhkmytKNU_Z2"
      },
      "source": [
        "<a name=\"experiments-and-discussion\"></a>\n",
        "# Experiments & Discussion\n",
        "\n",
        "In this section, describe your experiments and results. Briefly describe the performance metrics used and the significance of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LWo6UQzwj37"
      },
      "outputs": [],
      "source": [
        "# Insert code here. Feel free to break this up into several code\n",
        "# cells, interleaved with explanatory text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHIjM6eMwlXY"
      },
      "source": [
        "Finally, include a discussion on the limitations and important takeaways from the exercise.\n",
        "\n",
        "## Limitations\n",
        "*  Reflect on the potential biases or problems in the analysis presented in your tutorial, including its potential societal impact, and discuss how you might go about addressing this challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLLCQpv14Gsx"
      },
      "source": [
        "## Next Steps\n",
        "*   What do you imagine would be the next steps for your readers after finishing your tutorial?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkKqewK_-ZLD"
      },
      "source": [
        "<a name=\"references\"></a>\n",
        "# References\n",
        "\n",
        "*   EarthCube Notebook Template: https://github.com/earthcube/NotebookTemplates\n",
        "*   Earth Engine Community Tutorials Style Guide: https://developers.google.com/earth-engine/tutorials/community/styleguide#colab\n",
        "*   Google Cloud Community Tutorial Style Guide: https://cloud.google.com/community/tutorials/styleguide\n",
        "*   Rule A, Birmingham A, Zuniga C, Altintas I, Huang S-C, Knight R, et al. (2019) Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks. PLoS Comput Biol 15(7): e1007007. https://doi.org/10.1371/journal.pcbi.1007007\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVxrY8eNY-Lw"
      },
      "source": [
        "# Submitting the Tutorial *(Please remove this section from your submission.)*\n",
        "\n",
        "If you are using Google Colab, make sure to change the permissions by clicking \"Share\" (upper right corner of the notebook) >> Change permissions to \"Anyone on the internet with this link can comment\".\n",
        "\n",
        "See our website for additional instructions:\n",
        "https://sites.google.com/view/tafm\n",
        "\n",
        "For additional questions, please feel free to contact:\n",
        "*   tafm.rlc@gmail.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ2VeDQSL3sV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e722ae09a5a14157970c46f2e3b08391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdf0df1095374a90acd29e4f255ab930",
              "IPY_MODEL_53980ea5405b408fa1e5f505fd28fa46",
              "IPY_MODEL_8e1a30b21ae441789ab3651f5dc9d163"
            ],
            "layout": "IPY_MODEL_7544b9820b8b4ef39bb9fbadbe1bdf4e"
          }
        },
        "cdf0df1095374a90acd29e4f255ab930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dcd3b799bb14c7dbb8c0c0cc489df83",
            "placeholder": "​",
            "style": "IPY_MODEL_1d2e7a6cada54851a280cd27485baf47",
            "value": "100%"
          }
        },
        "53980ea5405b408fa1e5f505fd28fa46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22bc819a91184af98a891de3ae06dd76",
            "max": 900,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd7212aed09346ba846031cd53195fe2",
            "value": 900
          }
        },
        "8e1a30b21ae441789ab3651f5dc9d163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76f65f3cd64843b99042dca78fe938d1",
            "placeholder": "​",
            "style": "IPY_MODEL_f4503c9de9584500bd6aa15f579bfbfb",
            "value": " 900/900 [00:06&lt;00:00, 171.89it/s]"
          }
        },
        "7544b9820b8b4ef39bb9fbadbe1bdf4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dcd3b799bb14c7dbb8c0c0cc489df83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d2e7a6cada54851a280cd27485baf47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22bc819a91184af98a891de3ae06dd76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd7212aed09346ba846031cd53195fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76f65f3cd64843b99042dca78fe938d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4503c9de9584500bd6aa15f579bfbfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f170cd8066fa4b228a316d2eae351b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d268349a55e48b6821aa4a50b9d3ceb",
              "IPY_MODEL_679a338a56d44adfb6af1ebc2b1f043d",
              "IPY_MODEL_4994efa3a0e24a84bbb0a03b67a35c18"
            ],
            "layout": "IPY_MODEL_5cd71e1265704657a20d8e5d922d0c4e"
          }
        },
        "7d268349a55e48b6821aa4a50b9d3ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95490ad4aa984102b1419310bd6fa06e",
            "placeholder": "​",
            "style": "IPY_MODEL_fc01a8795f4a48999603c91d4f304649",
            "value": "Map: 100%"
          }
        },
        "679a338a56d44adfb6af1ebc2b1f043d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5adc966f91a49f8a750d2324f836d4e",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5aa7604932224ff79e16a099c95bef35",
            "value": 500
          }
        },
        "4994efa3a0e24a84bbb0a03b67a35c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcef2d01eac54be68a6b5d0b4e2de535",
            "placeholder": "​",
            "style": "IPY_MODEL_50f24095301f446888527def2caf97dd",
            "value": " 500/500 [00:00&lt;00:00, 1265.33 examples/s]"
          }
        },
        "5cd71e1265704657a20d8e5d922d0c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95490ad4aa984102b1419310bd6fa06e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc01a8795f4a48999603c91d4f304649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5adc966f91a49f8a750d2324f836d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aa7604932224ff79e16a099c95bef35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dcef2d01eac54be68a6b5d0b4e2de535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50f24095301f446888527def2caf97dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2701380c28242f6912c6e9de46464cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05399666d71a4e9999e026d0930a3d96",
              "IPY_MODEL_285ca7055af4423a92bfcb5f0f9ddc7a",
              "IPY_MODEL_b36d1b6b46d6496f8ace404df5846ba9"
            ],
            "layout": "IPY_MODEL_8e8f3846c9844a4cbb33b6c9cfb59382"
          }
        },
        "05399666d71a4e9999e026d0930a3d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cb8a24addeb4b928aa08ef55a700698",
            "placeholder": "​",
            "style": "IPY_MODEL_c506beec4d2a43d0a95ab39df2f936bd",
            "value": "100%"
          }
        },
        "285ca7055af4423a92bfcb5f0f9ddc7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_270e08503afe4e6bbf7402a78ab29c97",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ad319cccf334c20a963a5074bf031fa",
            "value": 500
          }
        },
        "b36d1b6b46d6496f8ace404df5846ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7c0730b21d4e9ab76d8fdd6aa2ce85",
            "placeholder": "​",
            "style": "IPY_MODEL_0f4bc6cb808341c9aea5065accb66dc9",
            "value": " 500/500 [03:23&lt;00:00,  2.06it/s]"
          }
        },
        "8e8f3846c9844a4cbb33b6c9cfb59382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cb8a24addeb4b928aa08ef55a700698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c506beec4d2a43d0a95ab39df2f936bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "270e08503afe4e6bbf7402a78ab29c97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ad319cccf334c20a963a5074bf031fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c7c0730b21d4e9ab76d8fdd6aa2ce85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f4bc6cb808341c9aea5065accb66dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}